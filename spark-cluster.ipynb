{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707cc39c-31f3-408e-a61f-8fd612627910",
   "metadata": {},
   "source": [
    "## Spark cluster (standalone) - Prediction notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95249bb5-5ed3-4682-aaba-3f6fa14fe30a",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Dockerized env : [JupyterLab server => Spark (master <-> 1 worker) ]  \n",
    "`docker-compose.yml` was (slightly) adapted from this [article](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3)  \n",
    "\n",
    "> Original notebook is heavily modified :  \n",
    "-random forest regressor instead of the article's linreg  \n",
    "-use of a Pipeline (pyspark.ml.pipeline) to streamline the whole prediction process  \n",
    "-no more sql-type queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f3fab-f7e5-4ec1-a985-3e7636e89b79",
   "metadata": {},
   "source": [
    "#### Connect to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d190ed50-ee85-4001-aede-b7632295ebd0",
   "metadata": {},
   "source": [
    "Reminder (as defined in docker-compose.yml) :\n",
    "- This notebook : http://localhost:8888\n",
    "- Access Master http://localhost:8080\n",
    "- Access Worker http://localhost:8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac3e8958-5d9c-4e80-9a6f-fd343a3d4dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml\")\n",
    "    .config(\"executor.memory\", \"4g\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85224eff-bc45-4a1a-ad75-859c3c5a6973",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run example - pyspark.sql / pyspark.ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba80b72-4efc-4369-9acc-525613671e7b",
   "metadata": {},
   "source": [
    "On Avocado dataset (how original). If you cloned git repo, is in /data, else go Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd61e63-f050-4eca-a3ba-206d34f93980",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401c9d4-128b-47f0-a53e-10eb2bff7b20",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Quick desc / scope of dataset :*  \n",
    "No EDA, this exercise have been made a million times\n",
    "Years 2015 to 2018  \n",
    "Two avocado types : organic or conventional  \n",
    "Region = region of consumption  \n",
    "Avocado sizes (PLU): 4046 (small-medium), 4225 (large), 4770 (x-large), expressed in total # of sold avocados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "888a85f7-5e40-4e90-8a35-3cb1435d1460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- AveragePrice: double (nullable = true)\n",
      " |-- Total Volume: double (nullable = true)\n",
      " |-- 4046: double (nullable = true)\n",
      " |-- 4225: double (nullable = true)\n",
      " |-- 4770: double (nullable = true)\n",
      " |-- Total Bags: double (nullable = true)\n",
      " |-- Small Bags: double (nullable = true)\n",
      " |-- Large Bags: double (nullable = true)\n",
      " |-- XLarge Bags: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|_c0|               Date|AveragePrice|Total Volume|   4046|     4225| 4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|  0|2015-12-27 00:00:00|        1.33|    64236.62|1036.74| 54454.85|48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
      "|  1|2015-12-20 00:00:00|        1.35|    54876.98| 674.28| 44638.81|58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
      "|  2|2015-12-13 00:00:00|        0.93|   118220.22|  794.7|109149.67|130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
      "|  3|2015-12-06 00:00:00|        1.08|    78992.15| 1132.0| 71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache table/dataframe for re-usable table with .cache()\n",
    "# caching operation takes place only when a Spark action (count, show, take or write) is also performed on the same dataframe\n",
    "df_avocado = spark.read.csv(\n",
    "  \"data/avocado.csv\", \n",
    "  header=True, \n",
    "  inferSchema=True\n",
    ").cache() # cache transformation\n",
    "\n",
    "df_avocado.printSchema()\n",
    "df_avocado.show(4) # call show() from the cached df_avocado. df_avocado cached in memory right after we call the action (show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c92350-8990-4534-81cc-4c9da00a40b1",
   "metadata": {},
   "source": [
    "#### Steps overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4734d8a-4399-4980-82c3-05940fbf888e",
   "metadata": {},
   "source": [
    "- No EDA, has been done a million times, we jump to the implementation directly\n",
    "- feature creation from 'Date' : --> yy and mm\n",
    "- one hot encoding categorical 'region'\n",
    "- scale numerical features ()\n",
    "- Drop columns : _c0 (index), Total Bags, Total Volume (strong corr with respective subcategories)\n",
    "- Drop transformed columns (if not aready done):  Date, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95442a63-b7d7-409a-95f2-6650cbe6bc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
